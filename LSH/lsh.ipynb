{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing LSH in Python\n",
    "\n",
    "Tutorial is modified from (https://www.learndatasci.com/tutorials/building-recommendation-engine-locality-sensitive-hashing-lsh-python/) by **Ray McLendon**. In its original tutorial, top-k nearest was reported to the user using MinHash Forest. In this tutorial, however, we are more keen to see how MinHash and LSH work.\n",
    "\n",
    "We will be using the API MinHash and MinHashLSH which is described in (http://ekzhu.com/datasketch/documentation.html).\n",
    "\n",
    "MinHash is the minHash we learned.\n",
    "\n",
    "MinHashLSH is the LSH index. \n",
    "\n",
    "# Step 1: Load Python Packages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasketch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d7d23cff7e87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasketch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMinHash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMinHashLSH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasketch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from datasketch import MinHash, MinHashLSH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Exploring Your Data\n",
    "\n",
    "Our goal in this tutorial is to make recommendations on conference papers by using LSH to quickly query all of the known conference papers. As a general rule, you should always examine your data. You need a thorough understanding of the dataset in order to properly pre-process your data and determine the best parameters. We have given some basic guidelines for selecting parameters, and they all require an exploration of your dataset as described above.\n",
    "\n",
    "\n",
    "For the purposes of this tutorial, we will be working with an easy dataset. Kaggle has the \"Neural Information Processing Systems (NIPS) conference papers. You can find them [here](http://www.kaggle.com/benhamner/nips-papers).\n",
    "\n",
    "\n",
    "An initial data exploration of these papers can be found [here](http://www.kaggle.com/benhamner/exploring-the-nips-papers).\n",
    "\n",
    "\n",
    "# Step 3: Preprocess your data\n",
    "\n",
    "For the purposes of this article, we will only use a rough version of unigrams as our shingles. We use the following steps:\n",
    "\n",
    "\n",
    "1. Remove all punctuation.\n",
    "1. Lowercase all text.\n",
    "1. Create unigram shingles (tokens) by separating any white space.\n",
    "\n",
    "For better results, you may try using a natural language processing library like NLTK or spaCy to produce unigrams and bigrams, remove stop words, and perform lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess will split a string of text into individual tokens/shingles based on whitespace.\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    tokens = text.lower()\n",
    "    tokens = tokens.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The devil went down to Georgia'\n",
    "print('The shingles (tokens) are:', preprocess(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Choose your parameters\n",
    "To start our example, we will use the standard number of permutations of 128. We will also start by just making one recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Permutations\n",
    "permutation = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Define the minHash function\n",
    "\n",
    "We define a function call myMinHash that will put all tokens into a MinHash object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myMinHash(tokens, perms):\n",
    "    m = MinHash(num_perm=perms)\n",
    "    for s in tokens:\n",
    "        m.update(s.encode('utf8'))\n",
    "    return m\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Evaluate Queries\n",
    "\n",
    "We will start by loading the CSV containing all the conference papers and creating a new field that combines the title and the abstract into one field, so we can build are shingles using both title and abstract.\n",
    "\n",
    "Finally, we can query any string of text such as a title or general topic to return a list of recommendations. Note, for our example below, we have actually picked the title of a conference paper. Naturally, we get the exact paper as one of our recommendations. We will follow the steps below:\n",
    "\n",
    "1. Preprocess your text into shingles.\n",
    "1. Set the same number of permutations for MinHash.\n",
    "1. Build the MinHashLSH with all MinHash.\n",
    "1. Query the forest with your MinHash and return the number of requested recommendations.\n",
    "1. Provide the titles of each conference paper recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.read_csv('papers.csv')\n",
    "\n",
    "#Prepare a field called data that contains the shingles of the title\n",
    "db['data']=db['title'].apply(preprocess)\n",
    "\n",
    "#Create a minHash for each row and save it into the column mHash\n",
    "db['mhash'] = db['data'].apply(myMinHash, args=(permutation,))\n",
    "\n",
    "#Create a MinHashLSH object called lsh. The params say I have 32 bands of 4 rows\n",
    "lsh = MinHashLSH(num_perm=permutation, params=[32,4])\n",
    "db.apply(lambda x: lsh.insert(x['id'], x['mhash']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create some titles that are similar to the paper. See if the algorithm will be able to retrive some almost the same papers\n",
    "titles = {'Self-Organization of Associative Database and Its Applications',\n",
    "          'Self-Organization of Associative Database and  Applications',\n",
    "          'Self-Organization of Associative Database ants Applications',\n",
    "          'Self-Organization of Associative Database d Its Applications',\n",
    "          'Self-Organization of Associative Datase and Its Applications',\n",
    "          'Self-Organization of and Its Applications'\n",
    "        }\n",
    "\n",
    "for title in titles:\n",
    "    a = preprocess(title)\n",
    "    mhash = minHash(a, permutation)\n",
    "\n",
    "    u = lsh.query(mhash)\n",
    "    \n",
    "    idx_array = np.array(u)\n",
    "    result = db.loc[db['id'].isin(idx_array)]['title']\n",
    "    \n",
    "    print('\\n\\nTitle', title, '\\t number of matches:', len(result), '\\n---------------------------')\n",
    "    for i in result:\n",
    "        print(i)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
